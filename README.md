# Introduction

An original implementation of the paper [Attention is All You
Need](https://arxiv.org/pdf/1706.03762.pdf) by Vaswani et al.

![Preliminary Loss](assets/loss_small.png)

This is a work in progress.  Above is shown preliminary training on a 1% shard of the
WMT 2014 English German Translation data set.



